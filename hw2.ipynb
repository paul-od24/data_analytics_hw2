{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Analytics Homework 2 - David Mallon (08597596) & Paul O'Donovan (22208104)</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a data quality report for your CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'patsy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpatches\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpatsy\u001b[39;00m \u001b[39mimport\u001b[39;00m dmatrices\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'patsy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a csv file, into a data frame\n",
    "df1 = pd.read_csv('covid19-cdc-22208104.csv', keep_default_na=True, delimiter=',', engine='python', skipinitialspace=True)\n",
    "\n",
    "# Reading from a csv file, into a data frame\n",
    "df2 = pd.read_csv('covid19-cdc-8597596.csv', keep_default_na=True, delimiter=',', engine='python', skipinitialspace=True)\n",
    "\n",
    "# Concatenate the data frames\n",
    "df = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns: (40000, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>state_fips_code</th>\n",
       "      <th>res_county</th>\n",
       "      <th>county_fips_code</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>case_positive_specimen_interval</th>\n",
       "      <th>case_onset_interval</th>\n",
       "      <th>process</th>\n",
       "      <th>exposure_yn</th>\n",
       "      <th>current_status</th>\n",
       "      <th>symptom_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>underlying_conditions_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04</td>\n",
       "      <td>IL</td>\n",
       "      <td>17.0</td>\n",
       "      <td>COOK</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04</td>\n",
       "      <td>MA</td>\n",
       "      <td>25.0</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>25025.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11</td>\n",
       "      <td>PA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>42101.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12</td>\n",
       "      <td>NY</td>\n",
       "      <td>36.0</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>36081.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>AZ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>4013.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Probable Case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_month res_state  state_fips_code    res_county  county_fips_code  \\\n",
       "0    2021-04        IL             17.0          COOK           17031.0   \n",
       "1    2020-04        MA             25.0       SUFFOLK           25025.0   \n",
       "2    2020-11        PA             42.0  PHILADELPHIA           42101.0   \n",
       "3    2021-12        NY             36.0        QUEENS           36081.0   \n",
       "4    2022-01        AZ              4.0      MARICOPA            4013.0   \n",
       "\n",
       "        age_group     sex   race            ethnicity  \\\n",
       "0  50 to 64 years    Male  White      Hispanic/Latino   \n",
       "1       65+ years    Male  White  Non-Hispanic/Latino   \n",
       "2       65+ years  Female  White              Unknown   \n",
       "3       65+ years  Female  Asian  Non-Hispanic/Latino   \n",
       "4       65+ years    Male  White  Non-Hispanic/Latino   \n",
       "\n",
       "   case_positive_specimen_interval  case_onset_interval  process exposure_yn  \\\n",
       "0                              1.0                  0.0  Missing     Missing   \n",
       "1                              0.0                  NaN  Missing     Missing   \n",
       "2                              0.0                  NaN  Missing         Yes   \n",
       "3                              0.0                  NaN  Missing     Missing   \n",
       "4                              NaN                  NaN  Missing     Missing   \n",
       "\n",
       "              current_status symptom_status  hosp_yn   icu_yn death_yn  \\\n",
       "0  Laboratory-confirmed case        Missing      Yes  Missing      Yes   \n",
       "1  Laboratory-confirmed case        Missing      Yes  Missing      Yes   \n",
       "2  Laboratory-confirmed case    Symptomatic  Unknown  Unknown      Yes   \n",
       "3  Laboratory-confirmed case        Unknown      Yes      Yes      Yes   \n",
       "4              Probable Case        Missing  Missing  Missing      Yes   \n",
       "\n",
       "  underlying_conditions_yn  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      Yes  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many rows and columns this dataframe has\n",
    "print(\"Number of rows and columns:\", df.shape)\n",
    "\n",
    "# Show first 5 rows of data frame\n",
    "# The rows are indexed starting from 0\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40000 entries, 0 to 19999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   case_month                       40000 non-null  object \n",
      " 1   res_state                        39999 non-null  object \n",
      " 2   state_fips_code                  39999 non-null  float64\n",
      " 3   res_county                       37702 non-null  object \n",
      " 4   county_fips_code                 37702 non-null  float64\n",
      " 5   age_group                        39740 non-null  object \n",
      " 6   sex                              39234 non-null  object \n",
      " 7   race                             35303 non-null  object \n",
      " 8   ethnicity                        34875 non-null  object \n",
      " 9   case_positive_specimen_interval  20865 non-null  float64\n",
      " 10  case_onset_interval              17552 non-null  float64\n",
      " 11  process                          40000 non-null  object \n",
      " 12  exposure_yn                      40000 non-null  object \n",
      " 13  current_status                   40000 non-null  object \n",
      " 14  symptom_status                   40000 non-null  object \n",
      " 15  hosp_yn                          40000 non-null  object \n",
      " 16  icu_yn                           40000 non-null  object \n",
      " 17  death_yn                         40000 non-null  object \n",
      " 18  underlying_conditions_yn         3495 non-null   object \n",
      "dtypes: float64(4), object(15)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Get a summary of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>state_fips_code</th>\n",
       "      <td>39999.0</td>\n",
       "      <td>29.556239</td>\n",
       "      <td>13.341753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county_fips_code</th>\n",
       "      <td>37702.0</td>\n",
       "      <td>29578.939605</td>\n",
       "      <td>13211.869250</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>18163.0</td>\n",
       "      <td>34017.0</td>\n",
       "      <td>37183.0</td>\n",
       "      <td>56039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_positive_specimen_interval</th>\n",
       "      <td>20865.0</td>\n",
       "      <td>0.194201</td>\n",
       "      <td>2.312371</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_onset_interval</th>\n",
       "      <td>17552.0</td>\n",
       "      <td>-0.057714</td>\n",
       "      <td>1.800305</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   count          mean           std     min  \\\n",
       "state_fips_code                  39999.0     29.556239     13.341753     1.0   \n",
       "county_fips_code                 37702.0  29578.939605  13211.869250  1001.0   \n",
       "case_positive_specimen_interval  20865.0      0.194201      2.312371   -90.0   \n",
       "case_onset_interval              17552.0     -0.057714      1.800305   -60.0   \n",
       "\n",
       "                                     25%      50%      75%      max  \n",
       "state_fips_code                     19.0     34.0     37.0     78.0  \n",
       "county_fips_code                 18163.0  34017.0  37183.0  56039.0  \n",
       "case_positive_specimen_interval      0.0      0.0      0.0     94.0  \n",
       "case_onset_interval                  0.0      0.0      0.0     69.0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the above results, I am missing 2298 pieces of information for county_fips_code. This is fine in my opinion as I have the almost full 39999 pieces of information for state_fips_code which will suffice for the purposes of this examination. The county_fips_code is not crucial and is sufficiently replaced and supplemented by the state_fips_code data.\n",
    "\n",
    "In relation to the missing / blank data for case_positive_specimen_interval, I will consider both sets of data in section 2 (my data quality plan) as I will consider their inclusion, possible amendments/corrections and any links that can be made with other provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_month', 'res_state', 'state_fips_code', 'res_county',\n",
       "       'county_fips_code', 'age_group', 'sex', 'race', 'ethnicity',\n",
       "       'case_positive_specimen_interval', 'case_onset_interval', 'process',\n",
       "       'exposure_yn', 'current_status', 'symptom_status', 'hosp_yn', 'icu_yn',\n",
       "       'death_yn', 'underlying_conditions_yn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at column names. Ensure there are not spaces in or after the name\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the state_fips_code and county_fips_code data represents a continuous range of values, but instead represent specific states and counties respectively, which are both categorical attributes. Both these datasets are better treated as categorical variables, which will help us understand the distribution of the data and any relationships or patterns that may exist between the state codes and other variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40000 entries, 0 to 19999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype   \n",
      "---  ------                           --------------  -----   \n",
      " 0   case_month                       40000 non-null  category\n",
      " 1   res_state                        39999 non-null  category\n",
      " 2   state_fips_code                  39999 non-null  category\n",
      " 3   res_county                       37702 non-null  category\n",
      " 4   county_fips_code                 37702 non-null  category\n",
      " 5   age_group                        39740 non-null  category\n",
      " 6   sex                              39234 non-null  category\n",
      " 7   race                             35303 non-null  category\n",
      " 8   ethnicity                        34875 non-null  category\n",
      " 9   case_positive_specimen_interval  20865 non-null  float64 \n",
      " 10  case_onset_interval              17552 non-null  float64 \n",
      " 11  process                          40000 non-null  category\n",
      " 12  exposure_yn                      40000 non-null  category\n",
      " 13  current_status                   40000 non-null  category\n",
      " 14  symptom_status                   40000 non-null  category\n",
      " 15  hosp_yn                          40000 non-null  category\n",
      " 16  icu_yn                           40000 non-null  category\n",
      " 17  death_yn                         40000 non-null  category\n",
      " 18  underlying_conditions_yn         3495 non-null   category\n",
      "dtypes: category(17), float64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "df[\"state_fips_code\"] = df[\"state_fips_code\"].astype(\"object\")\n",
    "df[\"county_fips_code\"] = df[\"county_fips_code\"].astype(\"object\")\n",
    "\n",
    "#Select all columns of type 'object'\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "\n",
    "#Convert selected columns to type 'category' purely for ease of reference when reviewing categorical and continuous characteristics.\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the features to their appropriate data types (e.g., decide which features are more appropriate as continuous and which ones as categorical types)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplicate rows and columns. Consider whether it makes sense to keep them or drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate (excluding first original occurence) rows in the table is:  3639\n",
      "Number of duplicate rows (including first original occurence) in the table is: 6178\n"
     ]
    }
   ],
   "source": [
    "#Print the number of duplicates, without the original rows that were duplicated\n",
    "print('Number of duplicate (excluding first original occurence) rows in the table is: ', df.duplicated().sum())\n",
    "\n",
    "# Check for duplicate rows. \n",
    "# Use \"keep=False\" to mark all duplicates as true, including the original rows that were duplicated.\n",
    "print('Number of duplicate rows (including first original occurence) in the table is:', df[df.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_duplicates = df[df.duplicated(keep = False)]\n",
    "view_duplicates.to_csv('duplicate_columns.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having reviewed the duplicates (as set out in a separate csv I prepared), I was satisfied that these should stay in the dataframe. As the information contained therein has been intentionally anonymised, there are not enough specific identifiers relating to each patient's data which would justify removing the duplicates. I believe it better to err on the side of caution and not remove what could (and probably is) information recorded which is identical but actually relates to different individuals, rather than remove valid information on the assumption that it is a duplicate entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      case_month res_state state_fips_code   res_county county_fips_code  \\\n",
      "235      2020-03        NY            36.0       QUEENS          36081.0   \n",
      "274      2021-01        CA             6.0  LOS ANGELES           6037.0   \n",
      "289      2020-12        CA             6.0  LOS ANGELES           6037.0   \n",
      "373      2021-01        FL            12.0   MIAMI-DADE          12086.0   \n",
      "476      2021-12        IL            17.0         COOK          17031.0   \n",
      "...          ...       ...             ...          ...              ...   \n",
      "19933    2020-07        TX            48.0      TARRANT          48439.0   \n",
      "19934    2022-01        IN            18.0       MARION          18097.0   \n",
      "19941    2021-12        NJ            34.0        OCEAN          34029.0   \n",
      "19944    2022-01        IN            18.0       GIBSON          18051.0   \n",
      "19972    2022-01        MN            27.0     HENNEPIN          27053.0   \n",
      "\n",
      "            age_group      sex     race            ethnicity  \\\n",
      "235         65+ years   Female    Black  Non-Hispanic/Latino   \n",
      "274         65+ years     Male    White      Hispanic/Latino   \n",
      "289    18 to 49 years     Male    White      Hispanic/Latino   \n",
      "373         65+ years   Female    Black  Non-Hispanic/Latino   \n",
      "476         65+ years     Male    White  Non-Hispanic/Latino   \n",
      "...               ...      ...      ...                  ...   \n",
      "19933  50 to 64 years     Male  Unknown              Unknown   \n",
      "19934  18 to 49 years   Female    Black  Non-Hispanic/Latino   \n",
      "19941  50 to 64 years   Female    White  Non-Hispanic/Latino   \n",
      "19944  18 to 49 years     Male    White  Non-Hispanic/Latino   \n",
      "19972  18 to 49 years  Unknown  Unknown              Missing   \n",
      "\n",
      "       case_positive_specimen_interval  case_onset_interval  process  \\\n",
      "235                                0.0                  NaN  Missing   \n",
      "274                                NaN                  NaN  Missing   \n",
      "289                                NaN                  NaN  Missing   \n",
      "373                                0.0                  NaN  Missing   \n",
      "476                                1.0                  0.0  Missing   \n",
      "...                                ...                  ...      ...   \n",
      "19933                              NaN                  NaN  Missing   \n",
      "19934                              0.0                  NaN  Missing   \n",
      "19941                              0.0                  NaN  Missing   \n",
      "19944                              0.0                  NaN  Missing   \n",
      "19972                              0.0                  NaN  Missing   \n",
      "\n",
      "      exposure_yn             current_status symptom_status  hosp_yn   icu_yn  \\\n",
      "235       Missing  Laboratory-confirmed case        Unknown      Yes  Unknown   \n",
      "274       Missing  Laboratory-confirmed case        Unknown      Yes  Unknown   \n",
      "289       Missing  Laboratory-confirmed case        Unknown      Yes      Yes   \n",
      "373       Missing  Laboratory-confirmed case        Missing      Yes  Missing   \n",
      "476       Missing  Laboratory-confirmed case        Missing      Yes  Missing   \n",
      "...           ...                        ...            ...      ...      ...   \n",
      "19933     Missing  Laboratory-confirmed case        Missing  Missing  Missing   \n",
      "19934     Missing  Laboratory-confirmed case        Missing       No  Missing   \n",
      "19941     Missing  Laboratory-confirmed case        Missing       No  Missing   \n",
      "19944     Missing  Laboratory-confirmed case        Missing       No  Missing   \n",
      "19972     Missing              Probable Case        Missing  Missing  Missing   \n",
      "\n",
      "      death_yn underlying_conditions_yn  \n",
      "235        Yes                      Yes  \n",
      "274        Yes                      NaN  \n",
      "289        Yes                      NaN  \n",
      "373        Yes                      NaN  \n",
      "476        Yes                      NaN  \n",
      "...        ...                      ...  \n",
      "19933       No                      NaN  \n",
      "19934       No                      NaN  \n",
      "19941       No                      NaN  \n",
      "19944       No                      NaN  \n",
      "19972       No                      NaN  \n",
      "\n",
      "[3639 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# quick view of duplicates\n",
    "\n",
    "duplicate = df[df.duplicated()]\n",
    " \n",
    "print(duplicate)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if any duplicate columns present.\n",
    "\n",
    "Function sourced from https://www.geeksforgeeks.org/how-to-find-drop-duplicate-columns-in-a-pandas-dataframe/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicateColumns(df):\n",
    "    duplicateColumnNames = set()\n",
    " \n",
    "    for x in range(df.shape[1]):\n",
    "        col = df.iloc[:, x]\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            otherCol = df.iloc[:, y]\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColNames = getDuplicateColumns(df)\n",
    "\n",
    "for column in duplicateColNames:\n",
    "    print('Column Name : ', column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate (excluding first) columns in the table is:  0\n",
      "Number of duplicate (including first) columns in the table is:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate columns - additional test\n",
    "#First transpose the df so columns become rows, then apply the same check as above\n",
    "dfT = df.T\n",
    "print(\"Number of duplicate (excluding first) columns in the table is: \", dfT.duplicated().sum())\n",
    "print(\"Number of duplicate (including first) columns in the table is: \",  dfT[dfT.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for constant columns. Consider whether it makes sense to keep them or drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns:  []\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique values in each column\n",
    "nunique_values = df.nunique()\n",
    "\n",
    "constant_columns = nunique_values[nunique_values == 1].index\n",
    "constant_column_names = list(constant_columns)\n",
    "\n",
    "# Print the names of the constant columns\n",
    "print(\"Constant columns: \", constant_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns:  []\n"
     ]
    }
   ],
   "source": [
    "c = [c for c in df.columns if len(set(df[c])) == 1]\n",
    "print (\"Constant columns: \", c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no 100% constant columns present in my dataset, therefore no adjustment is required.\n",
    "I have rechecked the data basing it on a 90% occurrence (when excluding empty cells) for completeness per the below cells and I would opine that the information in both the 'process' and 'underlying_conditions_yn' cells warrant inclusion in the cleaned dataframe due to weighing up the results against the number of empty cells with missing information. Further examination of these features is undertaken in part two of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns:  ['process', 'underlying_conditions_yn']\n"
     ]
    }
   ],
   "source": [
    "#Calculate the percentage of each value in each column, including empty cells\n",
    "value_percentages = df.apply(lambda x: x.value_counts(normalize=True, dropna=False).max())\n",
    "\n",
    "#Select only the columns where the maximum value percentage is >= 0.95\n",
    "constant_columns = value_percentages[value_percentages >= 0.90].index\n",
    "\n",
    "#List the names of the constant columns\n",
    "constant_column_names = list(constant_columns)\n",
    "\n",
    "#Print the names of the constant columns\n",
    "print(\"Constant columns: \", constant_column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information from both 'process' and 'underlying_conditions_yn' is still useful for consideration based on the above commentary. This information will also be useful when preparing my data quality plan and expected results from the bar plots. Further commentary is provided on my logic is provided in both my Data Quality Report and Data Quality Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Missing                            36463\n",
       "Clinical evaluation                 1663\n",
       "Laboratory reported                  730\n",
       "Routine surveillance                 460\n",
       "Multiple                             292\n",
       "Contact tracing of case patient      134\n",
       "Unknown                              113\n",
       "Provider reported                     80\n",
       "Other                                 64\n",
       "Autopsy                                1\n",
       "Name: process, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['process'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    3444\n",
       "No       51\n",
       "Name: underlying_conditions_yn, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['underlying_conditions_yn'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case_onset_interval - Drop column as this feature has little effect on target outcome <br>\n",
    "case_positive_specimen_interval - Drop column as this feature has little effect on target outcome<br>\n",
    "res_county - Drop column as it is like a duplicate feature to county_fips_code and it is not a unique identifier (eg county with same name in different states).<br>\n",
    "process - Drop column as this feature has little effect on target outcome and has most values missing.<br>\n",
    "exposure_yn - Drop column as this feature has little effect on target outcome and has most values missing.<br>\n",
    "symptom_status - Drop column as this feature has little effect on target outcome and is missing >50% of values<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert unknown and null to missing, and take missing values into account when analysing data for the following; <br>\n",
    "age_group<br>\n",
    "sex<br>\n",
    "race <br>\n",
    "ethnicity<br>\n",
    "hosp_yn<br>\n",
    "icu_yn<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert null to missing, and take missing values into account when analysing data \n",
    "underlying_conditions_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the specified columns\n",
    "df = df.drop(columns=[\"case_onset_interval\", \"case_positive_specimen_interval\", \"res_county\", \"process\", \"exposure_yn\", \"symptom_status\", \"state_fips_code\"])\n",
    "# replace \"unknown\" and \"null\" values with NaN\n",
    "df = df.replace([\"unknown\", \"null\"], np.nan)\n",
    "# replace \"null\" values with NaN\n",
    "df[\"underlying_conditions_yn\"] = df[\"underlying_conditions_yn\"].replace(\"null\", np.nan)\n",
    "df.to_csv('cleaned_data_hw2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q. Save your updated/cleaned data frame to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   case_month                40000 non-null  category\n",
      " 1   res_state                 39999 non-null  category\n",
      " 2   county_fips_code          37702 non-null  category\n",
      " 3   age_group                 39740 non-null  category\n",
      " 4   sex                       39234 non-null  category\n",
      " 5   race                      35303 non-null  category\n",
      " 6   ethnicity                 34875 non-null  category\n",
      " 7   current_status            40000 non-null  category\n",
      " 8   hosp_yn                   40000 non-null  category\n",
      " 9   icu_yn                    40000 non-null  category\n",
      " 10  death_yn                  40000 non-null  category\n",
      " 11  underlying_conditions_yn  3495 non-null   category\n",
      "dtypes: category(12)\n",
      "memory usage: 555.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Reading cleaned data into a data frame\n",
    "df = pd.read_csv('cleaned_data_hw2.csv')\n",
    "\n",
    "#Select all columns of type 'object'\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "\n",
    "#Convert selected columns to type 'category' purely for ease of reference when reviewing categorical and continuous characteristics.\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in this homework is to work with the data to build and evaluate prediction models that capture the relationship between the descriptive features and the target feature **death_yn**. For this homework you are asked to use the same dataset allocated to you in Homework1 (you can use your cleaned/prepared CSV from Homework1 or start from the raw dataset, clean it according to concepts covered in the lectures/labs, then use it for training prediction models). To use the 2 individual files allocated for Homework1, you can merge them first, then clean the resulting dataset, before starting on Homework2 requirements.\n",
    " \n",
    "There are 5 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 25% shown as [25]. The total that can be achieved is 100.\n",
    "\n",
    "\n",
    "(1). [25] **Data Understanding and Preparation:** Exploring relationships between feature pairs and selecting/transforming promising features based on a given training set.\n",
    "\n",
    "    - (1.1) Split the dataset into two datasets: 70% training and 30% test. Keep the test set aside. \n",
    "    - (1.2) On the training set:\n",
    "        - Plot the correlations between all the continuous features (if any). Discuss what you observe in these plots.\n",
    "        - For each continuous feature, plot its interaction with the target feature (a plot for each pair of   continuous feature and target feature). Discuss what you observe from these plots, e.g., which continuous features seem to be better at predicting the target feature? Choose a subset of continuous features you find promising (if any). Justify your choices.\n",
    "        - For each categorical feature, plot its pairwise interaction with the target feature. Discuss what  knowledge you gain from these plots, e.g., which categorical features seem to be better at predicting the target feature? Choose a subset of categorical features you find promising (if any). Justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>county_fips_code</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>current_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>underlying_conditions_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04</td>\n",
       "      <td>IL</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04</td>\n",
       "      <td>MA</td>\n",
       "      <td>25025.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11</td>\n",
       "      <td>PA</td>\n",
       "      <td>42101.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12</td>\n",
       "      <td>NY</td>\n",
       "      <td>36081.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>AZ</td>\n",
       "      <td>4013.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Probable Case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_month res_state  county_fips_code       age_group     sex   race  \\\n",
       "0    2021-04        IL           17031.0  50 to 64 years    Male  White   \n",
       "1    2020-04        MA           25025.0       65+ years    Male  White   \n",
       "2    2020-11        PA           42101.0       65+ years  Female  White   \n",
       "3    2021-12        NY           36081.0       65+ years  Female  Asian   \n",
       "4    2022-01        AZ            4013.0       65+ years    Male  White   \n",
       "\n",
       "             ethnicity             current_status  hosp_yn   icu_yn death_yn  \\\n",
       "0      Hispanic/Latino  Laboratory-confirmed case      Yes  Missing      Yes   \n",
       "1  Non-Hispanic/Latino  Laboratory-confirmed case      Yes  Missing      Yes   \n",
       "2              Unknown  Laboratory-confirmed case  Unknown  Unknown      Yes   \n",
       "3  Non-Hispanic/Latino  Laboratory-confirmed case      Yes      Yes      Yes   \n",
       "4  Non-Hispanic/Latino              Probable Case  Missing  Missing      Yes   \n",
       "\n",
       "  underlying_conditions_yn  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      Yes  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading cleaned data into a data frame\n",
    "df = pd.read_csv('cleaned_data_hw2.csv')\n",
    "df.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the cleaned csv, as a reminder we will check the shape, inspect the datatypes and check for any remaining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 12)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_month                   object\n",
       "res_state                    object\n",
       "county_fips_code            float64\n",
       "age_group                    object\n",
       "sex                          object\n",
       "race                         object\n",
       "ethnicity                    object\n",
       "current_status               object\n",
       "hosp_yn                      object\n",
       "icu_yn                       object\n",
       "death_yn                     object\n",
       "underlying_conditions_yn     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_month                      0\n",
       "res_state                       1\n",
       "county_fips_code             2298\n",
       "age_group                     260\n",
       "sex                           766\n",
       "race                         4697\n",
       "ethnicity                    5125\n",
       "current_status                  0\n",
       "hosp_yn                         0\n",
       "icu_yn                          0\n",
       "death_yn                        0\n",
       "underlying_conditions_yn    36505\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_month                    35\n",
      "res_state                     49\n",
      "county_fips_code            1340\n",
      "age_group                      5\n",
      "sex                            4\n",
      "race                           8\n",
      "ethnicity                      4\n",
      "current_status                 2\n",
      "hosp_yn                        4\n",
      "icu_yn                         4\n",
      "death_yn                       2\n",
      "underlying_conditions_yn       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_values = df.nunique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datatypes for plotting later\n",
    "We will now review the datatypes and convert if needed. This will help avoid plotting errors later in the notebook\n",
    "- The target feature \"death_yn\" is type object, with values \"Good\" & \"Bad\". These will be mapped 'yes': 1, \"no\": 0 and stored as \"int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert death_yn to 0,1\n",
    "df['death_yn'] = df['death_yn'].map({'yes': 1, \"no\": 0})\n",
    "\n",
    "# convert object columns to categorical\n",
    "df[\"case_month\"] = df[\"case_month\"].astype(\"category\")\n",
    "df[\"res_state\"] = df[\"res_state\"].astype(\"category\")\n",
    "df[\"age_group\"] = df[\"age_group\"].astype(\"category\")\n",
    "df[\"sex\"] = df[\"sex\"].astype(\"category\")\n",
    "df[\"race\"] = df[\"race\"].astype(\"category\")\n",
    "df[\"ethnicity\"] = df[\"ethnicity\"].astype(\"category\")\n",
    "df[\"current_status\"] = df[\"current_status\"].astype(\"category\")\n",
    "df[\"hosp_yn\"] = df[\"hosp_yn\"].astype(\"category\")\n",
    "df[\"icu_yn\"] = df[\"icu_yn\"].astype(\"category\")\n",
    "df[\"death_yn\"] = df[\"death_yn\"].astype(\"category\")\n",
    "df[\"underlying_conditions_yn\"] = df[\"underlying_conditions_yn\"].astype(\"category\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). [15] **Predictive Modeling:** Linear Regression.  \n",
    "\n",
    "    - (2.1) On the training set, train a linear regression model to predict the target feature, using only the  descriptive features selected in exercise (1) above. \n",
    "    - (2.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature).    \n",
    "    - (2.3) Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (2.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated random train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3). [15] **Predictive Modeling:** Logistic Regression.  \n",
    "\n",
    "    - (3.1) On the training set, train a logistic regression model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (3.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model).    \n",
    "    - (3.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (3.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). [20] **Predictive Modeling:** Random Forest.  \n",
    "\n",
    "    - (4.1) On the training set, train a random forest model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (4.2) Can you interpret the random forest model? Discuss any knowledge you can gain in regard of the working of this model.   \n",
    "    - (4.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (4.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and to the Random Forest out-of-sample error and discuss your findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5). [25] **Improving Predictive Models.**\n",
    "\n",
    "    - (5.1) Which model of the ones trained above performs better at predicting the target feature? Is it more   accurate than a simple model that always predicts the majority class (i.e., if 'no' is the majority class in your dataset, the simple model always predicts 'no' for the target feature)? Justify your answers.\n",
    "    - (5.2) Summarise your understanding of the problem and of your predictive modeling results so far. Can you think of any new ideas to improve the best model so far (e.g., by using furher data prep such as: feature selection, feature re-scaling, creating new features, combining predictive models, or using other domain knowledge)? Please show how your ideas actually work in practice (with code), by training and evaluating your proposed models. Summarise your findings so far. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47350py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13632a0cd2d42cf1a3d33146a97bdf8a96371b300caad9729a7c49f72e2a7611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
